<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tips Tricks &amp; Quick Fix</title>
    <description>Tips Tricks &amp; Quick Fix by &lt;a href=&#39;http://twitter.com/pietropassarell&#39; target=&#39;_blank&#39;&gt;@pietropassarell&lt;/a&gt;i.</description>
    <link></link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Aligning partially scripted speeches, R&amp;D Notes</title>
        <description>&lt;h1 id=&quot;use-case&quot;&gt;Use case&lt;/h1&gt;

&lt;p&gt;When working with the Vox Product Team last year, as part of the Knight-Mozilla fellowship with Open News, while making &lt;a href=&quot;http://pietropassarelli.com/autoEdit2.html&quot;&gt;autoEdit.io&lt;/a&gt; I had a chance to talk with many video producers across all brands.&lt;/p&gt;

&lt;p&gt;Vox.com video producers, when making explainers videos often rely on scripted voice over narration. Which means that they write their voice over (almost like an essay) and then add images, scenes, and interview sounds bits to it in the video editing software of choice (or sometimes they’d paper-edit this in google docs).&lt;/p&gt;

&lt;p&gt;When using autoEdit.io, a fast text based video editing mac os x app, that leverages STT systems to edit video interviews, they pointed out that they would have rather have the possibility to align the scripted voiceover narration (which is about 80% of the spoken words in the piece on average) and have speech to text recognition only on the remaining “off script” parts. Eg interviews with contributors etc.. Rather than have to correct inaccurate speech to text on the voice over script parts for which they already had accurate text.&lt;/p&gt;

&lt;p&gt;This use case stood out and was left unresolved due to the complexity of the problem and the lack of available tooling for an efficient solution.&lt;/p&gt;

&lt;p&gt;However it’s a use case that will resonate with much of radio/tv/video production where you are looking to do aligning partially scripted speeches. In fact recently was asked about this via email, my response was long I decided to convert it into this blog post to see what are other people thoughts on this.&lt;/p&gt;

&lt;h1 id=&quot;forced-aligners-aeneas-vs-gentle&quot;&gt;Forced aligners: Aeneas Vs Gentle&lt;/h1&gt;

&lt;p&gt;I played around with both Aeneas and Gentle, which are two of the more popular open source forced aligner systems out there. If you know others, please do let me know. I guess they have their pros and cons depending on what you are trying to do. But for now I am currently biased towards Aeneas.&lt;/p&gt;

&lt;p&gt;Altho Aeneas is not able to recognise text that is not been provided to the system, while Gentle can do that to a certain extent.&lt;/p&gt;

&lt;p&gt;The key difference is the underlying implementation.&lt;/p&gt;

&lt;p&gt;You provide text to align and audio to both.&lt;/p&gt;

&lt;h2 id=&quot;gentle&quot;&gt;Gentle&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Gentle&lt;/strong&gt;, could also work without the text input, and work as a speech to text service, which is how I’ve integrate it in &lt;a href=&quot;http://pietropassarelli.com/autoEdit2.html&quot;&gt;auotEdit.io&lt;/a&gt;, a text based video editing app I worked on last year.&lt;/p&gt;

&lt;p&gt;Here the details of the setup/integration between the two, unfortunately is packaged as a server so &lt;a href=&quot;https://pietropassarelli.gitbooks.io/autoedit2-user-manual/content/setup-stt-apis/setup-stt-apis-gentle.html&quot;&gt;needs to be open as a separate app&lt;/a&gt;, altho some people had some luck running it with docker.&lt;/p&gt;

&lt;p&gt;However as force aligner, gentle, first does STT, using Kaldi, and a model created by the community around this tool, and then tries to match the generated text with the text you have provided.&lt;/p&gt;

&lt;p&gt;This is time consuming because you have to wait for the STT recognition. Which is roughly same length of the media, but on the bright side, it means it can recognise text that you did not provide.&lt;/p&gt;

&lt;p&gt;However, out of the box or with some more digging in the Gentle python code, you could use it to figure out which sections are “off script” and which once re not. With that distinction in place, you could then run the “off script” parts against a better quality STT system to get even better results.&lt;/p&gt;

&lt;h3 id=&quot;gentlekaldi-system-language-model&quot;&gt;Gentle/Kaldi system language model&lt;/h3&gt;

&lt;p&gt;As a side note, Gentle STT model is not as great as it could be with more training etc… but of the reasons why I integrated it with autoEdit.io as an open source option is that under the hood it uses Kaldi for the language model and STT. Which is the same system used by BBC R&amp;amp;D for the BBC in house STT. Which has been fed media from the BBC Archive and I’ve been told give pretty good results because of the extra training.&lt;/p&gt;

&lt;p&gt;So with a better language model Gentle could be much better as well.&lt;/p&gt;

&lt;h2 id=&quot;aeneas&quot;&gt;Aeneas&lt;/h2&gt;

&lt;p&gt;**Aeneas **on the other hand, is very fast, because the underlying implementation is different. And is also very very well documented. It was only after a read through the documentation I really understood some of the more interesting use cases, such as possibility for word level alignment, and how to improve performance by switching to a higher quality TTS.&lt;/p&gt;

&lt;p&gt;Aeneas given the text and audio input, converts the text to audio. doing TTS and then compares the two waveforms. Which is how it figures out the timecodes etc.. Apparently this is a process that is more accurate at line level then at word level.&lt;/p&gt;

&lt;p&gt;If you knew which sections are “off script”, with Aeneas is also possible to pass in a start and end time-code of the section you want to align. And only align a certain section. Because under the hood it uses ffmpeg.&lt;/p&gt;

&lt;p&gt;(btw there’s also &lt;a href=&quot;https://github.com/sillsdev/aeneas-installer/releases&quot;&gt;an easier bundled up way to install aeneas and it’s dependencies for mac and windows&lt;/a&gt;  which is not of much use if you are running it on a linux server)&lt;/p&gt;

&lt;h1 id=&quot;possible-implementation&quot;&gt;Possible Implementation&lt;/h1&gt;

&lt;p&gt;A possible implementation could be in &lt;strong&gt;3 parts&lt;/strong&gt; + some optimisation tricks&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Part 1, identify what parts are “of script”,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Part 2 align the scripted parted with original text,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Part 3 do a STT for “off script parts”.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For part one, you can use a Gentle type of approach, and identify the sections that are “off script” and those that are not.&lt;/p&gt;

&lt;p&gt;With that info you can then do part two with Aeneas only on the “on script” sections, and part 3 run the “off script” sections through STT, a high quality commercial one, IBM Watson STT, Google Cloud Speech, Microsoft Bing, Speech Matics etc.. or just use the Gentle results if they are good enough for your use case&lt;/p&gt;

&lt;p&gt;Obviously I strongly suggest converting the original media you are trying to align to audio at the beginning of the process, because it then becomes a lot faster to do all these splitting etc..&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/OpenNewsLabs/autoEdit_2/tree/master/lib/interactive_transcription_generator/transcriber&quot;&gt;The transcriber module in autoEdit&lt;/a&gt; and some of its components could be of interest for some of these operations if you are working in node.&lt;/p&gt;

&lt;h2 id=&quot;optimization-1---speed&quot;&gt;Optimization 1 - Speed&lt;/h2&gt;

&lt;p&gt;To speed up part one and part 3 &lt;a href=&quot;https://github.com/OpenNewsLabs/autoEdit_2/blob/master/lib/interactive_transcription_generator/transcriber/index.js#L130&quot;&gt;as you can see in the autoEdit code&lt;/a&gt; if you chunk the audio you want to align into 5 or 1 min chunks when sending it to STT to get the transcription you can considerably speed up the process. This is how I got autoEdit to always transcribe any length o media in a 5 minute turnaround time, coz they can transcribed concurrently. If speed is not an issue tho, you could always optimised for this later.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/antiboredom/9bed969c8b2f89ea4b6c&quot;&gt;This optimization was originally an idea from Sam Lavigne&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;optimization-2---words&quot;&gt;Optimization 2 - words&lt;/h2&gt;

&lt;p&gt;If you are worried about chunking in the middle of a word, you can use ffmpeg, to detect silence, and try trim in between words as much as possible. But I’d leave that for later optimization. I think the &lt;a href=&quot;https://github.com/ftlabs/transcription-service&quot;&gt;FT Lab transcription service had played around with that idea&lt;/a&gt; see &lt;a href=&quot;https://pietropassarelli.gitbooks.io/textav/remote-presentations/transcription-service-at-the-ft.html&quot;&gt;textAV presentation here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;get-in-touch&quot;&gt;Get in touch&lt;/h2&gt;

&lt;p&gt;Thoughts? Ideas? Alternatives? You have tried this? Get in touch via email, or on twitter @pietropassarell or in the Hyperaud.io slack if you are a member.&lt;/p&gt;

</description>
        <pubDate>Thu, 09 Nov 2017 00:00:00 +0800</pubDate>
        <link>//aligning-partially-scripted-speeches.html</link>
        <guid isPermaLink="true">//aligning-partially-scripted-speeches.html</guid>
      </item>
    
      <item>
        <title>BBC #newsHack &#39;17 `@TranscriptionBot`</title>
        <description>&lt;h2 id=&quot;use-case&quot;&gt;Use case&lt;/h2&gt;
&lt;p&gt;Journalist working with audio interviews, recorded on their smartphone, for a text article.     &lt;br /&gt;
Our team got an honorable mention at the &lt;a href=&quot;https://medium.com/bbc-news-labs/12-ideas-from-our-conversational-user-interface-newshack-8c1ef22ff515&quot;&gt;BBC News Labs conversational user interface &lt;code class=&quot;highlighter-rouge&quot;&gt;#newsHACK&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;prototype&quot;&gt;Prototype&lt;/h2&gt;
&lt;p&gt;User uploads audio file onto slack channel, slack bot handles transcriptions, and allows user to query transcription, for questions, answers, both, and insight into the text.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/BBCnewslabs_slack_transcription_bot/1_add_audio.png&quot; alt=&quot;Slackbot Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;presentation&quot;&gt;Presentation&lt;/h2&gt;

&lt;div class=&quot;videoWrapper&quot;&gt;
	&lt;iframe src=&quot;https://player.vimeo.com/video/240180260?title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; width=&quot;100%&quot; height=&quot;400&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/1ky52Q9UJ9VV_gl6hKQWBS5GrOIARIqogwSZbkswXcY0/edit?usp=sharing&quot;&gt;slides&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1n6B1vyQysIEeDfoDUqnVNzb7RVy_MNuyvrVekLQ0YZ4/edit?usp=sharing&quot;&gt;R&amp;amp;D doc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See &lt;a href=&quot;OpenNewsLabs/chatbot_BBCNewsHack17&quot;&gt;github repo &lt;code class=&quot;highlighter-rouge&quot;&gt;README&lt;/code&gt; for more info&lt;/a&gt;.&lt;/p&gt;

&lt;!-- ## Screenshots

1_add_audio.png
2_play.png
3_summary.png
transcriptionSlackBotDemo1.gif
transcriptionSlackBotDemo2.gif

## Gifs
 --&gt;
</description>
        <pubDate>Thu, 26 Oct 2017 00:00:00 +0800</pubDate>
        <link>//BBCnewslabs_slack_transcription_bot.html</link>
        <guid isPermaLink="true">//BBCnewslabs_slack_transcription_bot.html</guid>
      </item>
    
      <item>
        <title>Fact2 Transcriptions Text Editor</title>
        <description>&lt;p&gt;Fact2 (pronounced Fast Squared) uses machine learning &amp;amp; neural networks to turn unstructured video, audio, text, PDFs to rich, searchable structured data.&lt;/p&gt;

&lt;p&gt;I was contracted to create an open source transcription text editor for their pool of proofreaders to correct the automated transcriptions generated by their system using speech to text services.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/pietrop/fact2_transcription_editor&quot;&gt;For github repo see here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/fact2_transcription_editor/Fact2_transcription_text_editor_shortcuts.png&quot; alt=&quot;Fact2 Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;key-features&quot;&gt;Key features&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;supports HTML5 audio and video &lt;br /&gt;
&lt;!-- add list --&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;hypertranscript: double click on a word takes you to corresponding part in the media (audio / video).
    &lt;ul&gt;
      &lt;li&gt;with customizable interval, default at 3 seconds before&lt;/li&gt;
      &lt;li&gt;in text indication of media time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rollback button, with custumizable variable, default at 15 seconds&lt;/li&gt;
  &lt;li&gt;puase while typing toggle, with customizable interval, default at 3 sec.&lt;/li&gt;
  &lt;li&gt;hide video toggle, (for when concentrating on transcribing/proofreading and it gets distracting)&lt;/li&gt;
  &lt;li&gt;jump to time&lt;/li&gt;
  &lt;li&gt;usal play, pause, rewind, forward, progress bar etc…&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Keyboard shortcuts, for all of above&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;speed controls, and display of playback rate&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;shortcut to display keyboard shortcuts, and display of keyboard shortcuts along side interface&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;seamless/plain text editing experience.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;support to
    &lt;ul&gt;
      &lt;li&gt;display confidence scores&lt;/li&gt;
      &lt;li&gt;display unaligned words&lt;/li&gt;
      &lt;li&gt;add and display speaker diarizaiton info, with button and shortcut&lt;/li&gt;
      &lt;li&gt;add description info eg &lt;code class=&quot;highlighter-rouge&quot;&gt;[inaudible]&lt;/code&gt;, with button and shortcut&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;mobile responsive (altho double tapping on words not enabled yet)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 11 Oct 2017 00:00:00 +0800</pubDate>
        <link>//fact2_transcription_editor.html</link>
        <guid isPermaLink="true">//fact2_transcription_editor.html</guid>
      </item>
    
      <item>
        <title>Video presentation: 10 Lessons from building video product tools in the newsroom.</title>
        <description>&lt;!-- &lt;iframe width=&quot;100%&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/jryiz5kC1V8?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt; --&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/234150382?title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; width=&quot;100%&quot; height=&quot;400&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Video presentation version of &lt;a href=&quot;/10-lessons-video-tools.html&quot;&gt;“10 Lessons from building video product tools in the newsroom”&lt;/a&gt; originally as a paper I’ve written for the computational journalism conference ‘17.&lt;/p&gt;
</description>
        <pubDate>Thu, 14 Sep 2017 00:00:00 +0800</pubDate>
        <link>//10-lessons-video-tools-ba.html</link>
        <guid isPermaLink="true">//10-lessons-video-tools-ba.html</guid>
      </item>
    
      <item>
        <title>10 Lessons from building video product tools in the newsroom.</title>
        <description>&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;With a unique background in Anthropology, Documentary film, and computer&lt;br /&gt;
science, I have developed a skill set of applying my skills and knowledge collectively and creatively. This article is to share the 10 lessons I learned from the development of autoEdit, and walked through how a mixed approach that combines lean, user-centered design and anthropological insights with key computer science techniques,  such as mapping the problem domain and component-based design, can be instrumental in developing products that gain user traction.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In 2016 I did a 10-month Knight-Mozilla Fellowship organised by Open News with the Vox Media products team in New York.&lt;/p&gt;

&lt;p&gt;My interest was to explore possibilities for open source tools to help streamline the video production process. After spending a month interviewing and observing video producers across Vox Media, I narrowed my focus down to creating an application that enables more efficient and accessible video editing of interviews.&lt;/p&gt;

&lt;p&gt;The final product is autoEdit (Passarelli 2016a), a Mac OS X desktop app that creates automatic transcription from a video or audio file. The user can make text selections and export those selections as a video sequence to the editing software of choice.&lt;/p&gt;

&lt;p&gt;The success of autoEdit was beyond my expectation - it was downloaded over 700 times within the first three months after its launch. It is currently being used by video producers and journalists across the Vox Media brands, at the Financial Times, CUNY Journalism School, Holocaust Memorial Museum, and many other companies and organizations.&lt;/p&gt;

&lt;p&gt;Following are 10 lessons I learned from building video product tools in the newsroom. The knowledge and expertise that lead me to approach product design the way I do is the result of my unusual background.&lt;/p&gt;

&lt;p&gt;My education background includes a B.A. in Anthropology from Goldsmiths, University of London, a MA in Documentary Films from London College of Communication, and a Msc in Computer Science from University College London. My professional background is also a unique mix, I worked in documentary production for BBC, channel 4, and discovery channel, I also had the opportunity to work as a newsroom developer at The Times &amp;amp; Sunday Times in London, where I created quickQuote, a tool that automatically generates interactive video quotes, which won Guardian’s Student Media Startup of the year award in 2015, and worked with the Guardian Video team.&lt;/p&gt;

&lt;p&gt;In the following section, I am going to distill the key takeaways for journalists and developers interested in using user-centered design and rapid prototyping to create products that gain user traction.&lt;/p&gt;

&lt;h3 id=&quot;user-research&quot;&gt;User Research&lt;/h3&gt;
&lt;p&gt;One of the biggest challenges when doing research to develop a new product is how to get useful actionable intel from users while iteratively developing a prototype.&lt;/p&gt;

&lt;p&gt;Let’s consider a few key ideas that can help when defining a course of action.&lt;/p&gt;

&lt;p&gt;The Lean methodology, pioneered by Eric Ries(Ries 2011), defines an approach to startup and product development initially inspired by the Toyota quick cycle of manufacturing. The focus is on learning, and to iteratively define and test hypothesis to expand understanding of the users and the problem we are trying to solve, rather than on building and deploying a polished product.&lt;/p&gt;

&lt;p&gt;This goes hand in hand with the idea that there are going to be early adopters that you should spend some time identifying as a defined group and focus your initial research on (Blank and Dorf 2012) .&lt;/p&gt;

&lt;p&gt;The idea of “participant observation” is a key element of ethnographic research in anthropology. In social sciences context you don’t just observe, measure and quantify, instead, you need a better way to gather qualitative insights in a rigorous way. Participant observations consist in embedding yourself with the subjects you are studying, taking part in their routines and activities to better understand the world from their point of view, and see what that reveals to you about their knowledge systems and beliefs.&lt;/p&gt;

&lt;p&gt;As anthropologist Bill Watson, from Kent University, often says, anthropological research has also a strong emphasis on looking at “what people do do, and not what they say they do”.&lt;/p&gt;

&lt;p&gt;Closely linked with these ideas is the concept of “The Mum test” (Fitzpatrick 2013) . In a nutshell, it says that to get actionable insights to the users you have to root the questions in the past. Using past behavior as more reliable predictor of future decisions. And avoid asking leading questions.&lt;/p&gt;

&lt;p&gt;For example if you want to figure out if your users would be interested in using automated transcriptions as a part of their video workflow, Rather than asking “Would you use an app that does X &amp;amp; Y?” first things first, you’d have to learn about their current workflow. “Tell me about the last project you worked on, what was that like from start to finish?”. It is then down to you to drill down to the granularity that gives you the most insights. Then you can ask what the biggest bottleneck is and what the users have done to try and simplify things.&lt;/p&gt;

&lt;p&gt;In user-centered design approach, building personas should not feel like a chore. Personas are abstract representations of users that embodies some of the common characteristics shared across your pool of early adopters. These are useful in the development and prototyping stage, when defining non-functional requirements.&lt;/p&gt;

&lt;p&gt;For instance, asking the users about their work, and what gives them the most satisfaction and what the most tedious part is, is also a great way to get a more rounded sense of who they are and how they think about things. This will also come in handy when considering how to position the product to them in a relatable way.&lt;/p&gt;

&lt;p&gt;Last but not least, anthropologists would encourage you to have a healthy distrusts for questionnaires as a research methodology to gather insights. However, if you use the “Mum Test” when phrasing the questions, it can be a useful tool to identify early adopters.&lt;/p&gt;

&lt;h3 id=&quot;be-a-domain-expert-but-if-you-are-not-thats-ok-too&quot;&gt;Be a domain expert, but if you are not that’s ok too.&lt;/h3&gt;
&lt;p&gt;A domain expert is someone who has expertise in a certain field, knows and understands the intricacies of contradictory and conflicting theories in that field, and can juggle those in their mind.&lt;/p&gt;

&lt;p&gt;For example, a youtuber might be an expert in doing social videos, but if they don’t know about ethics and other epistemological implications of the medium, they might not be a domain expert in documentary production.&lt;/p&gt;

&lt;p&gt;If you are not a domain expert in the area where you are looking to use user-centered design to build a successful product, that’s ok too, there’s a few ways you can make up for that.&lt;/p&gt;

&lt;p&gt;Once you have defined the domain you are going to be exploring, the first thing to figure out is what are considered as best practices, “expert view” on the topic. For instance in the area of using transcriptions for video production, a paper-edit is still considered as the best practice although having fallen out of fashion because of the tedious and time-consuming process in its analogue form.&lt;/p&gt;

&lt;p&gt;You also need to identify out who is regarded as expert, what are the different school of thoughts, etc. Then as a separate step, find out how your users feel about these experts and school of thoughts, which one they respect, which one they dismiss, and why. For example, in the case of the paper-editing workflow, most video producers either do a simplified variation of it, or don’t do it because it is too time consuming. While sometimes video editors dismiss it as working with text not being visual enough for their liking.&lt;/p&gt;

&lt;p&gt;Observe and learn from your users. Don’t take their word at face value, they might not always know what’s best for them. In this sense there is a strong parallel to documentary production where you have to research, analyse, interpret and draw your own informed conclusion to define a course of action. For example one of the feedback I had from a video producer at Vox about autoEdit is that it “Goes above and beyond what I thought was possible”.&lt;/p&gt;

&lt;h3 id=&quot;be-language-agnostic&quot;&gt;Be language agnostic&lt;/h3&gt;
&lt;p&gt;As a technologist, you should figure out the best technology to use for the problem you are trying to solve and to factor in the learning curve that comes with it.&lt;/p&gt;

&lt;p&gt;In computer science domain, It makes your life a lot easier if you have a focus on understanding the evergreen underlying knowledge, as it moves a lot slower than the latest hip framework that changes every 3 months.&lt;/p&gt;

&lt;p&gt;The tools and framework that we use shape the mental models we create to solve problems (Cook 2017), and some might be more suited than others. For instance, in the first version of autoEdit (Passarelli 2016a) I used ruby on rails with SQL as a database. This lead me to model the transcription after a captions, srt file, with line level granularity, which had severe limitation when trying to select text at word level. However in autoEdit2, when working with node and nosql databases, it was easier to reason at a word level granularity and enable more possibility for digital paper-editing (Passarelli 2016b).&lt;/p&gt;

&lt;p&gt;As the popular saying goes “if you only have a hammer everything becomes a nail”.&lt;/p&gt;

&lt;p&gt;Last but not least, Eric Ries has also argued that, in line with user-centered design you should also start with the users and the problem they are facing rather than trying to fit a specific technology to a specific problem (Ries 2011).&lt;/p&gt;

&lt;h3 id=&quot;map-the-problem-domain&quot;&gt;Map the problem domain&lt;/h3&gt;
&lt;p&gt;I also found it particularly helpful to map the problem domain as a strategy to deal with ever changing requirements (Winder and Roberts 2006).&lt;/p&gt;

&lt;p&gt;“A problem domain is the context in which a particular problem exists. For example, the problem domain in which a specific route plan exists is that of maps, route planning, travelling and strategies for moving around. Critically, the problem domain is relatively stable, changing only slowly, while specific problems to be solved are transient and change regularly. If you are able to capture the problem domain as the core of the design of your program, then the program code is likely to be more stable, more reusable and more easily adaptable to specific problems as they come and go.” (Winder and Roberts 2006, 351)&lt;/p&gt;

&lt;p&gt;“Consider wanting to know how to travel from A to B. You could ask someone for a route plan and get a list of instructions containing statements such as ‘go to the end of the road, turn left, then turn right at the third turning on the left’, and so on. With a basic understanding of how each statement is interpreted (e.g. how to count turnings to find the third on the left), you can follow the instructions and get to your destination”.(Winder and Roberts 2006, 354)&lt;/p&gt;

&lt;p&gt;“A given route plan may work but it is very specific. To find out how to travel to a different destination you have to go and ask for a new list of instructions every time you want to travel. An alternative strategy for dealing with your travelling problem is to make use of a map. This will allow you to travel between any two points on the map and do your own route planning. The route planning can even be done while you are travelling. There is a cost for using the map, you have to learn how to read and interpret it, as well as develop strategies for planning routes. However, in the longer term the benefits more than outweigh the initial learning costs. Moreover, much more of the map strategy is reusable—it is easy to transfer the solution strategy from one problem to the next.” (Winder and Roberts 2006, 355)&lt;/p&gt;

&lt;h3 id=&quot;use-component-based-design&quot;&gt;use component-based design&lt;/h3&gt;
&lt;p&gt;Another helpful technique is that of component-based design when writing the code of your program (Winder and Roberts 2006)&lt;/p&gt;

&lt;p&gt;“A component is typically implemented by a small collection of classes, with one class acting as an interface to the component. Rather than designing a program from scratch, it can be built out of a set of predefined components which are linked together with small amounts of new code. This approach is termed component-based design. The connections between components are enabled using the mechanisms of inheritance, interfaces and dynamic binding. Users of the component make use of the public interface to call the component’s methods but need not be aware of the details of the component implementation.” (Winder and Roberts 2006, 265)&lt;/p&gt;

&lt;p&gt;Thinking in terms of reusable components has several advantages. For instance autoEdit “front end” is written in web technologies in such a way that the demo on the project website is the same code but with a hard coded database. This means that if I were to make a web version of the app I could reuse that part with minimal adjustments. It ultimately comes down to making a judgement call between premature optimization vs being strategic. Often a thin line.&lt;/p&gt;

&lt;h3 id=&quot;use-an-rd-approach&quot;&gt;Use an R&amp;amp;D Approach&lt;/h3&gt;
&lt;p&gt;Once you have got some findings from initial user research, you are ready to take the next step to identify, learn, and understand possible workflows. Then you can treat it almost like a lego project.&lt;/p&gt;

&lt;p&gt;For example, for autoEdit, an understanding of the paper-editing workflow and how the users relate to that and/or a variation of it was crucial.&lt;/p&gt;

&lt;p&gt;To get a better feel for this, I run a series of workshop on paper-editing, where participants were learning interview story crafting and I would get the type of insights you normally get through focus groups, focusing on the underlying workflow and not the app itself. (Passarelli 2017) (Passarelli 2016c)&lt;/p&gt;

&lt;p&gt;Once you have the workflow figured out, next step is to divide it into parts. Of those parts identify the granularity of what are the components that make up that part. For those components, find out which ones you have available and to what degree you are familiar with their workings.&lt;/p&gt;

&lt;p&gt;As well as which ones you don’t know. Priority is given to the ones you don’t know to verify how they work and whether they make the whole possible.&lt;/p&gt;

&lt;p&gt;Once you have all the components you look at the interfaces and the communication between that, do you need to do any conversion or adjustment eg to get the output of one as input of the other etc.. This also helps you to think about data models representations that are most suited for the overall system.&lt;/p&gt;

&lt;p&gt;This allows to work on parts and components in isolation and then combined them. Don’t leave the combining and integration experiments too late tho because sometimes they are just as important as the building blocks.&lt;/p&gt;

&lt;p&gt;Threat everything as an hypothesis and prioritise which one to be tested first.&lt;/p&gt;

&lt;h3 id=&quot;learn-how-to-formulate-questions-as-a-way-to-get-over-unexpected-roadblocks-during-development&quot;&gt;Learn how to formulate questions as a way to get over unexpected roadblocks during development&lt;/h3&gt;
&lt;p&gt;It is not a question if you are going to get stuck, but rather a matter of when. It is therefore important to think about how you are going to get yourself out of it and use your time productively.&lt;/p&gt;

&lt;p&gt;Face with an unexpected problem, think about what you know, the limits of what you know, what you don’t know and the limits of what you don’t know. Then describe what you know about the current setup, and inner workings. Describe the symptoms of what’s not working as expected. Write it all down. This takes you already half way to find a solution, as you can now share it and communicate with others more efficiently when asking for advice. It’s important to also spend some time identifying the right vocabulary to express the problem. You can then iteratively make hypothesis about the root cause, and run experiments to test it out. This will help you narrow it down until you find a solution.&lt;/p&gt;

&lt;h3 id=&quot;be-brave-like-pixar&quot;&gt;Be brave like Pixar&lt;/h3&gt;
&lt;p&gt;I once watched this documentary about Pixar, The Pixar Story (Iwerks 2007), and was fascinated by how Toy Story team had the courage to throw away all the work just a couple of days before deadline, and started over from a blank canvas because the story and its main character was not quite working out.&lt;/p&gt;

&lt;p&gt;This might seem daunting but If you had mapped the problem domain, followed component-based architecture and user-centered design, you really are just throwing away only the material representation of your ideas. And you are actually in a good place to start over, with all the excitement and liberating anticipation of a blank canvas.&lt;/p&gt;

&lt;h3 id=&quot;human-vs-machine&quot;&gt;Human vs machine?&lt;/h3&gt;
&lt;p&gt;In the context of where to draw the line in the automation debate in replacing people’s jobs, especially with the recent buzz around AI, in studies that compared the performance of a team of humans, a team of “machines” and a mixed team of humans and computers, have found that the mixed team seems to consistently achieve the highest result (Allègre L and Matthias 2013).&lt;/p&gt;

&lt;p&gt;I’ve been dwelling on the ethical issues around the development of autoEdit, for a while now, there is a possibility that, especially if combined with more advanced cognitive services, it could replace video editors.&lt;/p&gt;

&lt;p&gt;After all IBM Watson can edit a video trailer (Smith 2016), with pretty good results.&lt;/p&gt;

&lt;p&gt;There are already examples of Earthquake bulletins written by machines without human review and the same has happened for economic reports (Jenkin 2016).&lt;/p&gt;

&lt;p&gt;In this context It’s important to recognise that there is always a bias, of some sort, gender, race etc.. of whoever wrote the algorithm (Devlin 2016) .&lt;/p&gt;

&lt;p&gt;In the current version of autoEdit, the aim was to remove the tedious parts of the paper-editing workflow to enable the video producers to concentrate on the story crafting.&lt;/p&gt;

&lt;h3 id=&quot;convenience-trumps-quality-every-day-of-the-week&quot;&gt;Convenience trumps quality every day of the week&lt;/h3&gt;
&lt;p&gt;In certain contexts of automation, convenience seems to win over quality every day of the week. Increasingly leading to a polarized scenario with either high volume production with low quality output or low quality production with high volume distribution, and no room for anything in between.&lt;/p&gt;

&lt;p&gt;An example is the use of text to speech to to do voice over of videos, where quality of the speech to text services is not as good as human narrators but can scale faster and cheaper.&lt;/p&gt;

&lt;p&gt;In the interest of adding value for the users, I would argue it is your job as an application developer to address both convenience and quality. Identify convenience hooks to get traction with your users, but be concerned about raising quality of the output. Don’t expect an increase in quality to be sufficient for new user adoption, however counterintuitive this might seem.&lt;/p&gt;

&lt;p&gt;For example, with autoEdit the 5 minutes turnaround time for transcriptions is a very important cutoff point that was instrumental in gaining traction with early adopters. And even automated speech to text service alone is not as good quality compared to transcriptions services with 24 hours turnaround time with services that provide human or human + machine transcriptions. But the convenience wins over quality. And therefore some users will put up with the decrease in quality of the transcription they get for the added value of a fast turnaround.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this article, we explored 10 lessons I learned from the development of autoEdit, and walked through how a mixed approach that combines lean, user-centered design and anthropological insights with key computer science techniques, such as mapping the problem domain and component-based design, can be instrumental in developing products that gain user traction. I would like to encourage readers to think deeper and further about how they can leverage some of the techniques I have explained to add value and increase quality of their products for their users.&lt;/p&gt;

&lt;h2 id=&quot;bibliography&quot;&gt;Bibliography&lt;/h2&gt;

&lt;p&gt;Allègre L,. Hadida, and Seifert Matthias. 2013. “3 Humans + 1 Computer = Best Prediction.” &lt;a href=&quot;https://hbr.org/2013/05/3-humans-1-computer-best-prediction&quot;&gt;https://hbr.org/2013/05/3-humans-1-computer-best-prediction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Blank, Steve, and Bob Dorf. 2012. The Startup Owner’s Manual: The Step-By-Step Guide for Building a Great Company. 1 edition. Pescadero, Calif: K &amp;amp; S Ranch.&lt;/p&gt;

&lt;p&gt;Cook, Blaine. 2017. “Annotations Models.” &lt;a href=&quot;https://pietropassarelli.gitbooks.io/textav/problem-domains/d83d-dd2a-2705-2b07-fe0f-annotations-models.html&quot;&gt;https://pietropassarelli.gitbooks.io/textav/problem-domains/d83d-dd2a-2705-2b07-fe0f-annotations-models.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Devlin, Hannah. 2016. “Discrimination by Algorithm: Scientists Devise Test to Detect AI Bias Technology The Guardian.”&lt;a href=&quot;https://www.theguardian.com/technology/2016/dec/19/discrimination-by-algorithm-scientists-devise-test-to-detect-ai-bias&quot;&gt; https://www.theguardian.com/technology/2016/dec/19/discrimination-by-algorithm-scientists-devise-test-to-detect-ai-bias&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Fitzpatrick, Rob. 2013. The Mom Test. 1 edition. CreateSpace Independent Publishing Platform.&lt;/p&gt;

&lt;p&gt;Iwerks, Leslie. 2007. “The Pixar Story: Leslie Iwerks: Amazon Digital Services LLC.” &lt;a href=&quot;https://www.amazon.com/Pixar-Story-Leslie-Iwerks/dp/B006DQP64A/ref=sr_1_fkmr0_1?ie=UTF8&amp;amp;qid=1501371860&amp;amp;sr=8-1-fkmr0&amp;amp;keywords=The+Pixar+Story+documentary+dvd&quot;&gt;https://www.amazon.com/Pixar-Story-Leslie-Iwerks/dp/B006DQP64A/ref=sr_1_fkmr0_1?ie=UTF8&amp;amp;qid=1501371860&amp;amp;sr=8-1-fkmr0&amp;amp;keywords=The+Pixar+Story+documentary+dvd&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jenkin, Matthew. 2016. “Written Out of the Story: The Robots Capable of Making the News Guardian Small Business Network The Guardian.” &lt;a href=&quot;https://www.theguardian.com/small-business-network/2016/jul/22/written-out-of-story-robots-capable-making-the-news&quot;&gt;https://www.theguardian.com/small-business-network/2016/jul/22/written-out-of-story-robots-capable-making-the-news&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Passarelli, Pietro. 2016a. “autoEdit.” &lt;a href=&quot;www.autoEdit.io&quot;&gt;www.autoEdit.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. 2016b. “autoEdit User Manual.” &lt;a href=&quot;https://pietropassarelli.gitbooks.io/autoedit2-user-manual/content/paperediting.html&quot;&gt;https://pietropassarelli.gitbooks.io/autoedit2-user-manual/content/paperediting.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. 2016c. “How to Craft Compelling Stories Out of Video Interviews?” &lt;a href=&quot;http://pietropassarelli.com/wip_london_july2016.html&quot;&gt;http://pietropassarelli.com/wip_london_july2016.html&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;———. 2017. How to Tell Compelling Stories Out of Video Interviews. &lt;a href=&quot;https://www.gitbook.com/book/pietropassarelli/how-to-tell-compelling-stories-out-of-video-inter/&quot;&gt;https://www.gitbook.com/book/pietropassarelli/how-to-tell-compelling-stories-out-of-video-inter/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Ries, Eric. 2011. The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses. 1 edition. New York: Crown Business.&lt;/p&gt;

&lt;p&gt;Smith, John R. 2016. “IBM Research Takes Watson to Hollywood with the First ‘Cognitive Movie Trailer’ - THINK Blog.” &lt;a href=&quot;https://www.ibm.com/blogs/think/2016/08/cognitive-movie-trailer/&quot;&gt;https://www.ibm.com/blogs/think/2016/08/cognitive-movie-trailer/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Winder, Russel, and Graham Roberts. 2006. Developing Java Software. 3 edition. Chichester, UK ; Hoboken, NJ: Wiley.&lt;/p&gt;
</description>
        <pubDate>Tue, 01 Aug 2017 00:00:00 +0800</pubDate>
        <link>//10-lessons-video-tools.html</link>
        <guid isPermaLink="true">//10-lessons-video-tools.html</guid>
      </item>
    
      <item>
        <title>Opened Captions Annotated article system</title>
        <description>&lt;!-- _draft: more coming soon_ --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://pietropassarelli.gitbooks.io/opened-captions-for-annotated-articles&quot;&gt;gitbook notes and video of presentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/1yI6SkJi-RqV11_fFImfh44iG011hPlgtwNzYcF2P1_U/edit?usp=sharing&quot;&gt;slides from SRCCON’17 presentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pietrop/Opened-Captions-for-Annotated-Articles&quot;&gt;github repo that syncs with gitbook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also recently made a working proof of concept for an app to replace the rig, to make it easier for the end user to use without the supervision of a newsroom developer. See screenshot below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/opened_caption_app/opened_caption_annotation_app.png&quot; alt=&quot;screenshot of layout &quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Mon, 24 Jul 2017 00:00:00 +0800</pubDate>
        <link>//opened-captions-annotated-article-rig.html</link>
        <guid isPermaLink="true">//opened-captions-annotated-article-rig.html</guid>
      </item>
    
      <item>
        <title>Captioning app [Working Title]</title>
        <description>&lt;!-- TODO move from  --&gt;

&lt;!-- _draft: more coming soon_ --&gt;

&lt;p&gt;&lt;img src=&quot;https://pbs.twimg.com/media/DF7ytQ7VoAA1ry2.jpg:large&quot; alt=&quot;captioning app screenshot&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the &lt;a href=&quot;http://pietropassarelli.com/textAV.html&quot;&gt;textAV&lt;/a&gt; event with a small team we did the &lt;a href=&quot;https://pietropassarelli.gitbooks.io/textav/content/unconference-projects/captioning-workflow-system.html&quot;&gt;requirements gathering &lt;/a&gt; based on &lt;a href=&quot;https://pietropassarelli.gitbooks.io/textav/content/remote-presentations/captioning-workflow.html&quot;&gt;Joseph Polizzotto’s presentation&lt;/a&gt; and &lt;a href=&quot;https://github.com/oTranscribe/oTranscribe&quot;&gt;oTranscribe&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the following weekend I built an app that considerably speeds up and simplify the captioning workflow.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It auto transcribes&lt;/li&gt;
  &lt;li&gt;It adds punctuation automatically&lt;/li&gt;
  &lt;li&gt;User can then correct the text&lt;/li&gt;
  &lt;li&gt;It auto segments and aligns to create caption file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;See &lt;a href=&quot;http://pietropassarelli.com/captions-maker&quot;&gt;demo here&lt;/a&gt; for demonstration purposes (click &lt;strong&gt;load&lt;/strong&gt; and then &lt;strong&gt;align&lt;/strong&gt;).&lt;/p&gt;

&lt;p&gt;It auto transcribes using speech to text service to give you a first draft, it auto adds the punctuations, you can review and correct, and when you click &lt;strong&gt;align&lt;/strong&gt; it segments and auto  aligns(very fast) to create a caption file (srt but could support more format) similar to youtube captioning editor.&lt;/p&gt;

&lt;p&gt;Gives you a preview of the caption file, with clicable timecodes that take you to that point in the video.&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://twitter.com/JPolizzotto&quot;&gt;@JPolizzotto&lt;/a&gt; &lt;a href=&quot;https://twitter.com/GidsG&quot;&gt;@GidsG&lt;/a&gt; auto transcribe, auto punctuation, auto srt lines alignment... &lt;a href=&quot;https://twitter.com/hashtag/textAV?src=hash&quot;&gt;#textAV&lt;/a&gt;.tech &lt;a href=&quot;https://t.co/mbW799QYnr&quot;&gt;pic.twitter.com/mbW799QYnr&lt;/a&gt;&lt;/p&gt;&amp;mdash; Pietro (@pietropassarell) &lt;a href=&quot;https://twitter.com/pietropassarell/status/889321437504385024&quot;&gt;July 24, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Mon, 24 Jul 2017 00:00:00 +0800</pubDate>
        <link>//captioning-app.html</link>
        <guid isPermaLink="true">//captioning-app.html</guid>
      </item>
    
      <item>
        <title>textAV event</title>
        <description>&lt;!-- _draft: more coming soon_ --&gt;

&lt;p&gt;More details &lt;br /&gt;
- &lt;a href=&quot;https://pietropassarelli.gitbooks.io/textav/content/&quot;&gt;Video presentations and notes from event&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;http://textAV.tech&quot;&gt;Event website&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;https://sites.google.com/view/textav/confirmed-participants&quot;&gt;Line up of participants&lt;/a&gt;&lt;br /&gt;
- &lt;a href=&quot;https://trello.com/b/tyIWiHOE/text-a-v-event-topics&quot;&gt;Trello board with projects&lt;/a&gt; &lt;br /&gt;
- &lt;a href=&quot;http://textav.hyperaud.io/pad&quot;&gt;hyperaud.io repo of the talks&lt;/a&gt;&lt;/p&gt;

&lt;!-- TODO: add Molly&#39;s post from Source when available.

TODO: add a bit about rational of the event, knowledge share to get people up to speed + unconference to hack on project. 

But not an hackaton, goal to grow and streghten the ecosystem.

 --&gt;

&lt;hr /&gt;

&lt;!--  


&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;What a morning view from &lt;a href=&quot;https://t.co/x9UQL7s0K4&quot;&gt;https://t.co/x9UQL7s0K4&lt;/a&gt; event space &lt;a href=&quot;https://twitter.com/NyuTisch&quot;&gt;@NyuTisch&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/TextAV?src=hash&quot;&gt;#TextAV&lt;/a&gt; &lt;a href=&quot;https://t.co/Y5ixZqk0Bd&quot;&gt;pic.twitter.com/Y5ixZqk0Bd&lt;/a&gt;&lt;/p&gt;&amp;mdash; Joscha Jaeger (@OpenHypervideo) &lt;a href=&quot;https://twitter.com/OpenHypervideo/status/887660076999335936&quot;&gt;July 19, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;


&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Lots of news bout &lt;a href=&quot;https://twitter.com/PopUpArchive&quot;&gt;@PopUpArchive&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/audiosearchfm&quot;&gt;@audiosearchfm&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/baileyspace&quot;&gt;@baileyspace&lt;/a&gt; at &lt;a href=&quot;https://twitter.com/hashtag/textav?src=hash&quot;&gt;#textav&lt;/a&gt;. 💁 &lt;a href=&quot;https://t.co/9hiTuGMzv4&quot;&gt;pic.twitter.com/9hiTuGMzv4&lt;/a&gt;&lt;/p&gt;&amp;mdash; Dave Rice (@dericed) &lt;a href=&quot;https://twitter.com/dericed/status/887739052182700032&quot;&gt;July 19, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;


&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Archive problems with &lt;a href=&quot;https://twitter.com/dericed&quot;&gt;@dericed&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/villereal&quot;&gt;@villereal&lt;/a&gt; at &lt;a href=&quot;https://twitter.com/hashtag/textav?src=hash&quot;&gt;#textav&lt;/a&gt; &lt;a href=&quot;https://t.co/ByyEa58k2W&quot;&gt;pic.twitter.com/ByyEa58k2W&lt;/a&gt;&lt;/p&gt;&amp;mdash; amymonte (@amymonte) &lt;a href=&quot;https://twitter.com/amymonte/status/887769074561937408&quot;&gt;July 19, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;


&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Yes here we go: &amp;quot;archive problems&amp;quot; with &lt;a href=&quot;https://twitter.com/dericed&quot;&gt;@dericed&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/villereal&quot;&gt;@villereal&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/textav?src=hash&quot;&gt;#textav&lt;/a&gt; &lt;a href=&quot;https://t.co/KO5yh7MuYP&quot;&gt;pic.twitter.com/KO5yh7MuYP&lt;/a&gt;&lt;/p&gt;&amp;mdash; Molly Schwartz (@mollyfication) &lt;a href=&quot;https://twitter.com/mollyfication/status/887768871448580096&quot;&gt;July 19, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;


&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;judge for yourself &lt;a href=&quot;https://t.co/5DaekoV4XE&quot;&gt;pic.twitter.com/5DaekoV4XE&lt;/a&gt;&lt;/p&gt;&amp;mdash; Dave Rice (@dericed) &lt;a href=&quot;https://twitter.com/dericed/status/887691814123261952&quot;&gt;July 19, 2017&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

--&gt;

&lt;!-- 


Captions burner 
https://voxmedia.github.io/captions_burner/ 
Needs testing with vtt to see if it supports styling. 


 --&gt;
</description>
        <pubDate>Wed, 19 Jul 2017 00:00:00 +0800</pubDate>
        <link>//textAV.html</link>
        <guid isPermaLink="true">//textAV.html</guid>
      </item>
    
      <item>
        <title>Gitbook: How to tell compelling stories out of video interviews</title>
        <description>&lt;p&gt;&lt;em&gt;draft: more coming soon&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;http://pietropassarelli.com/wip_london_july2016.html&quot;&gt;series of workshops&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;There is &lt;a href=&quot;https://pietropassarelli.gitbooks.io/how-to-tell-compelling-stories-out-of-video-inter/content/paper-editing/key-concepts-in-documentary-theory.html&quot;&gt;a section on documentary representation&lt;/a&gt; that I think it’s interesting/relevant in the accountability debate that has been raised with the new issues around fake news, the increasing need for transparency and interests by the viewers to see the source material.&lt;/p&gt;

&lt;!-- 


Captions burner 
https://voxmedia.github.io/captions_burner/ 
Needs testing with vtt to see if it supports styling. 


 --&gt;
</description>
        <pubDate>Fri, 07 Jul 2017 00:00:00 +0800</pubDate>
        <link>//paper-edit-gitbook.html</link>
        <guid isPermaLink="true">//paper-edit-gitbook.html</guid>
      </item>
    
      <item>
        <title>autoEdit2</title>
        <description>&lt;!-- _draft: more coming soon_ --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://source.opennews.org/articles/video-editing-made-better-introducing-autoedit&quot;&gt;SOURCE: ‘Introducing autoEdit: Video Editing Made Better’&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.niemanlab.org/2016/10/try-out-this-open-source-tool-for-editing-video-and-audio-transcripts&quot;&gt;NiemanLab: ‘Try out this open source tool for editing video and audio transcripts’&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.journalism.co.uk/news/tool-for-journalists-autoedit-to-edit-videos-quicker-using-transcriptions/s2/a684359/&quot;&gt;Journalism.co.uk: ‘Tool for journalists: AutoEdit, to edit videos quicker using transcriptions’&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://product.voxmedia.com/2016/11/22/13669486/faster-video-editing&quot;&gt;Vox Media Product Blog: ‘An open source tool for enabling faster, easier editing of video interviews’&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.rjionline.org/stories/fl171-ibm-watson-speech-to-text&quot;&gt;RJI: ‘FL#171: IBM Watson Speech to Text’&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://www.autoedit.io/img/autoEdit_overview_diagram_1.0.7.png&quot; alt=&quot;autoEdit&quot; /&gt;&lt;/p&gt;

&lt;!-- 

http://www.autoedit.io/

- http://www.niemanlab.org/2016/10/try-out-this-open-source-tool-for-editing-video-and-audio-transcripts/ 

- https://www.journalism.co.uk/news/tool-for-journalists-autoedit-to-edit-videos-quicker-using-transcriptions/s2/a684359/

- https://product.voxmedia.com/2016/11/22/13669486/faster-video-editing



TODO: link to computational journalism paper once you publish in blog section

 --&gt;
</description>
        <pubDate>Tue, 04 Apr 2017 00:00:00 +0800</pubDate>
        <link>//autoEdit2.html</link>
        <guid isPermaLink="true">//autoEdit2.html</guid>
      </item>
    
  </channel>
</rss>
